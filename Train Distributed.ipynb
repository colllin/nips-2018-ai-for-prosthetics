{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from models.td3 import TD3\n",
    "# from osim.env import ProstheticsEnv\n",
    "from environment.prosthetics_env_with_history import ProstheticsEnvWithHistory\n",
    "from environment.observations import prepare_model_observation, env_obs_history_to_model_obs\n",
    "from environment.actions import prepare_env_action, reset_frameskip\n",
    "from environment.rewards import env_obs_to_custom_reward\n",
    "from distributed.database import persist_timesteps, persist_event, get_total_timesteps\n",
    "from distributed.db_history_sampler import DatabaseHistorySampler\n",
    "from distributed.s3_checkpoints import load_s3_model_checkpoint, save_s3_model_checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"env\": {\n",
      "        \"integrator_accuracy\": 0.002\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"architecture\": \"TD3\"\n",
      "    },\n",
      "    \"rollout\": {\n",
      "        \"#\": \"Frameskip will be applied for random durations between 0 and `frameskip` timesteps.\",\n",
      "        \"max_episode_steps\": 600,\n",
      "        \"expl_noise\": 0.25,\n",
      "        \"frameskip\": 5\n",
      "    },\n",
      "    \"distributed\": {\n",
      "        \"policy_weights_dir_s3\": \"s3://colllin-nips-2018-prosthetics/checkpoints/\",\n",
      "        \"policy_weights_basename\": \"checkpoint_TD3\",\n",
      "        \"#\": \"How often (episodes) we download model weights during rollout.\",\n",
      "        \"rollout_refresh_model_freq\": 5\n",
      "    },\n",
      "    \"training\": {\n",
      "        \"#\": \"Frequency of delayed policy updates\",\n",
      "        \"eval_freq\": 5000.0,\n",
      "        \"batch_size\": 100,\n",
      "        \"discount\": 0.99,\n",
      "        \"tau\": 0.005,\n",
      "        \"policy_noise\": 0.2,\n",
      "        \"noise_clip\": 0.5,\n",
      "        \"policy_freq\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('config_distributed.json', 'r') as f:\n",
    "    CONFIG = json.load(f)\n",
    "print(json.dumps(CONFIG, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simulation env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = ProstheticsEnvWithHistory(visualize=False, integrator_accuracy=CONFIG['env']['integrator_accuracy'])\n",
    "env_step_kwargs = {'project': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Policy, Download & load latest weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 19, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dim = env.observation_space.shape[0]\n",
    "env.reset(**env_step_kwargs)\n",
    "state_dim = prepare_model_observation(env).shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = int(env.action_space.high[0])\n",
    "state_dim, action_dim, max_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy checkpoints from s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading policy checkpoints from {CONFIG['distributed']['policy_weights_dir_s3']}{CONFIG['distributed']['policy_weights_basename']}*\")\n",
    "load_s3_model_checkpoint(\n",
    "    policy, \n",
    "    s3_dir=CONFIG['distributed']['policy_weights_dir_s3'],\n",
    "    basename=CONFIG['distributed']['policy_weights_basename'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Hacking (Custom \"done\" criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_abort_episode(env_obs, custom_rewards=None, verbose=False):\n",
    "#     print((np.array(env_obs['body_pos_rot']['torso'])*180/math.pi > 60).any())\n",
    "#     if env_obs['body_pos_rot']['torso'][2] < -0.2:\n",
    "#         return True\n",
    "    rewards = custom_rewards if custom_rewards != None else env_obs_to_custom_reward(env_obs)\n",
    "    # print(f'Custom reward: {sum(rewards.values())}')\n",
    "    if (env_obs['body_pos']['head'][0] - env_obs['body_pos']['pelvis'][0]) < -.2:\n",
    "        if verbose: print(f'Aborting episode due to head being > .2m behind the pelvis ({env_obs[\"body_pos\"][\"head\"][0] - env_obs[\"body_pos\"][\"pelvis\"][0]}).')\n",
    "        return True\n",
    "    if np.fabs(env_obs['body_pos']['head'][2]) > 0.5:\n",
    "        if verbose: print(f'Aborting episode due to head being > 0.5m away from centerline ({env_obs[\"body_pos\"][\"head\"][2]}).')\n",
    "        return True\n",
    "    if sum(rewards.values()) < -10:\n",
    "        if verbose:\n",
    "            print(f'Aborting episode due to custom reward < -10 ({sum(rewards.values())}):')\n",
    "            for k,v in rewards.items():\n",
    "                if v < 0:\n",
    "                    print(f'  reward `{k}` = {v}')\n",
    "        return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns average reward\n",
    "def evaluate_episode(policy):\n",
    "    obs = env.reset(**env_step_kwargs)\n",
    "    reset_frameskip(0)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = policy.select_action(prepare_model_observation(env))\n",
    "        action = prepare_env_action(action)\n",
    "        obs, reward, done, _ = env.step(action, **env_step_kwargs)\n",
    "        \n",
    "        # We don't use the custom rewards here because we want to evaluate our progress against the environment's reward.\n",
    "        # obs_dict = env.get_state_desc()\n",
    "        # custom_rewards = compute_rewards(obs_dict)\n",
    "        # total_rewared += reward + sum(custom_rewards.values())\n",
    "\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.\n",
    "    for _ in tqdm(range(eval_episodes), desc=\"Evaluating policy\", unit=\"episode\"):\n",
    "        avg_reward += evaluate_episode(policy)\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Evaluation over %d episodes: %f\" % (eval_episodes, avg_reward))\n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "    return avg_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init database sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'df_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-440de8b91bc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menv_obs_history_to_model_obs_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_obs_history_to_model_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menv_obs_custom_reward_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_obs_to_custom_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0menv_obs_custom_done_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshould_abort_episode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'df_history'"
     ]
    }
   ],
   "source": [
    "history_sampler = DatabaseHistorySampler(\n",
    "    env_obs_history_to_model_obs_fn=env_obs_history_to_model_obs, \n",
    "    env_obs_custom_reward_fn=lambda obs: sum(env_obs_to_custom_reward(obs).values()),\n",
    "    env_obs_custom_done_fn=should_abort_episode,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Train for `eval_freq` batches:\n",
    "    if CONFIG['model']['architecture'] == \"TD3\":\n",
    "        policy.train(\n",
    "            history_sampler,#replay_buffer, \n",
    "            CONFIG['training']['eval_freq'],\n",
    "            CONFIG['training']['batch_size'], \n",
    "            CONFIG['training']['discount'], \n",
    "            CONFIG['training']['tau'], \n",
    "            CONFIG['training']['policy_noise'], \n",
    "            CONFIG['training']['noise_clip'], \n",
    "            CONFIG['training']['policy_freq'],\n",
    "        )\n",
    "    else: \n",
    "        policy.train(\n",
    "            history_sampler,#replay_buffer, \n",
    "            CONFIG['training']['eval_freq'],\n",
    "            CONFIG['training']['batch_size'], \n",
    "            CONFIG['training']['discount'], \n",
    "            CONFIG['training']['tau']\n",
    "        )\n",
    "        \n",
    "    # Evaluate Policy\n",
    "    eval_reward = evaluate_policy(policy)\n",
    "    \n",
    "    # Persist evalutation timesteps to central database. (Why not?)\n",
    "    persist_timesteps(env.history())\n",
    "    env.reset_history()\n",
    "    \n",
    "    # Log evaluation history\n",
    "    total_timesteps = get_total_timesteps()\n",
    "    persist_event('eval', f'Evaluated policy @ {total_timesteps} timesteps in DB, average reward: {eval_reward}')\n",
    "    \n",
    "    # Upload policy weights to S3, to be picked up by instances running the Rollout Distributed process.\n",
    "    upload_model_weights(policy, CONFIG['distributed']['policy_weights_basename'])\n",
    "    \n",
    "    # Also upload policy weights under unique name as a historical checkpoint.\n",
    "    evalname = f\"{CONFIG['distributed']['policy_weights_basename']}_T{total_timesteps}_eval{avg_reward}\"\n",
    "    print(f\"Loading policy checkpoints from {CONFIG['distributed']['policy_weights_dir_s3']}{CONFIG['distributed']['policy_weights_basename']}*\")\n",
    "    load_s3_model_checkpoint(\n",
    "        policy, \n",
    "        s3_dir=CONFIG['distributed']['policy_weights_dir_s3'],\n",
    "        basename=CONFIG['distributed']['policy_weights_basename'],\n",
    "    )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosthetics",
   "language": "python",
   "name": "prosthetics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
