{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from models.td3 import TD3\n",
    "# from osim.env import ProstheticsEnv\n",
    "from environment.prosthetics_env_with_history import ProstheticsEnvWithHistory\n",
    "from environment.observations import prepare_model_observation\n",
    "from environment.actions import prepare_env_action, reset_frameskip\n",
    "# from environment.rewards import env_obs_to_custom_reward\n",
    "from distributed.database import persist_timesteps, persist_event\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"env\": {\n",
      "        \"integrator_accuracy\": 0.002\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"architecture\": \"TD3\"\n",
      "    },\n",
      "    \"rollout\": {\n",
      "        \"#\": \"Frameskip will be applied for random durations between 0 and `frameskip` timesteps.\",\n",
      "        \"max_episode_steps\": 600,\n",
      "        \"expl_noise\": 0.25,\n",
      "        \"frameskip\": 5\n",
      "    },\n",
      "    \"distributed\": {\n",
      "        \"policy_weights_dir_s3\": \"s3://colllin-nips-2018-prosthetics/checkpoints/\",\n",
      "        \"policy_weights_basename\": \"checkpoint_TD3\",\n",
      "        \"#\": \"How often (episodes) we download model weights during rollout.\",\n",
      "        \"rollout_refresh_model_freq\": 5\n",
      "    },\n",
      "    \"training\": {\n",
      "        \"#\": \"Frequency of delayed policy updates\",\n",
      "        \"eval_freq\": 2500,\n",
      "        \"batch_size\": 100,\n",
      "        \"discount\": 0.99,\n",
      "        \"tau\": 0.005,\n",
      "        \"policy_noise\": 0.2,\n",
      "        \"noise_clip\": 0.5,\n",
      "        \"policy_freq\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('config_distributed.json', 'r') as f:\n",
    "    CONFIG = json.load(f)\n",
    "print(json.dumps(CONFIG, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simulation env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = ProstheticsEnvWithHistory(visualize=False, integrator_accuracy=CONFIG['env']['integrator_accuracy'])\n",
    "env_step_kwargs = {'project': False}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Hacking (Custom \"done\" criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def should_abort_episode(env_obs, custom_rewards=None, verbose=False):\n",
    "# #     print((np.array(env_obs['body_pos_rot']['torso'])*180/math.pi > 60).any())\n",
    "# #     if env_obs['body_pos_rot']['torso'][2] < -0.2:\n",
    "# #         return True\n",
    "#     rewards = custom_rewards if custom_rewards != None else env_obs_to_custom_reward(env_obs)\n",
    "#     # print(f'Custom reward: {sum(rewards.values())}')\n",
    "#     if (env_obs['body_pos']['head'][0] - env_obs['body_pos']['pelvis'][0]) < -.2:\n",
    "#         if verbose: print(f'Aborting episode due to head being > .2m behind the pelvis ({env_obs[\"body_pos\"][\"head\"][0] - env_obs[\"body_pos\"][\"pelvis\"][0]}).')\n",
    "#         return True\n",
    "#     if np.fabs(env_obs['body_pos']['head'][2]) > 0.5:\n",
    "#         if verbose: print(f'Aborting episode due to head being > 0.5m away from centerline ({env_obs[\"body_pos\"][\"head\"][2]}).')\n",
    "#         return True\n",
    "#     if sum(rewards.values()) < -10:\n",
    "#         if verbose:\n",
    "#             print(f'Aborting episode due to custom reward < -10 ({sum(rewards.values())}):')\n",
    "#             for k,v in rewards.items():\n",
    "#                 if v < 0:\n",
    "#                     print(f'  reward `{k}` = {v}')\n",
    "#         return True\n",
    "#     return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random rollout (Record & Persist Simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs_by_thread_id = {}\n",
    "\n",
    "def get_env_for_current_thread():\n",
    "    thread_id = id(multiprocessing.current_process())\n",
    "    print(thread_id)\n",
    "    if not thread_id in envs_by_thread_id:\n",
    "        envs_by_thread_id[thread_id] = ProstheticsEnvWithHistory(visualize=False, integrator_accuracy=CONFIG['env']['integrator_accuracy'])\n",
    "    return envs_by_thread_id[thread_id]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_random_episode(episode_num):\n",
    "    env = get_env_for_current_thread()\n",
    "    # Reset environment\n",
    "    obs = env.reset(**env_step_kwargs)\n",
    "    reset_frameskip(CONFIG['rollout']['frameskip'])\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "\n",
    "    while not done:\n",
    "        # Select action randomly or according to policy\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Perform action\n",
    "        action = prepare_env_action(action)\n",
    "        obs, reward, done, _ = env.step(action, **env_step_kwargs)\n",
    "\n",
    "        # if not done:\n",
    "        #     done = should_abort_episode(env.get_state_desc(), verbose=True)\n",
    "\n",
    "        # custom_rewards = compute_rewards(new_obs_dict)\n",
    "        episode_reward += reward #+ sum(custom_rewards.values())\n",
    "\n",
    "        episode_timesteps += 1\n",
    "\n",
    "    # Persist timesteps to central database\n",
    "    persist_timesteps(env.history())\n",
    "    env.reset_history()\n",
    "\n",
    "    # Log episode\n",
    "    persist_event('rollout_episode_completed', {\n",
    "        'episode_num': episode_num,\n",
    "        'episode_timesteps': episode_timesteps,\n",
    "        'episode_reward': episode_reward,\n",
    "    })\n",
    "    print(f\"Episode Num: {episode_num} Episode T: {episode_timesteps} Reward: {episode_reward}\")\n",
    "    sys.stdout.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139696887843584\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode Num: 0 Episode T: 86 Reward: -335.37909307949167\n",
      "139696887843584\n",
      "Episode Num: 1 Episode T: 105 Reward: -532.6242029128832\n",
      "139696887843584\n",
      "Episode Num: 2 Episode T: 59 Reward: -41.40560493443378\n",
      "139696887843584\n",
      "Episode Num: 3 Episode T: 91 Reward: -506.8561854425691\n",
      "139696887843584\n",
      "Episode Num: 4 Episode T: 100 Reward: -546.7786659979884\n",
      "139696887843584\n",
      "Episode Num: 5 Episode T: 89 Reward: -361.7370415300716\n",
      "139696887843584\n",
      "Episode Num: 6 Episode T: 88 Reward: -320.73696562512\n",
      "139696887843584\n",
      "Episode Num: 7 Episode T: 95 Reward: -562.4698248097595\n",
      "139696887843584\n",
      "Episode Num: 8 Episode T: 80 Reward: -21.851426105068168\n",
      "139696887843584\n",
      "Episode Num: 9 Episode T: 94 Reward: -510.8689696292805\n",
      "139696887843584\n",
      "Episode Num: 10 Episode T: 56 Reward: -18.09804525454996\n",
      "139696887843584\n",
      "Episode Num: 11 Episode T: 95 Reward: -448.71823693079864\n",
      "139696887843584\n",
      "Episode Num: 12 Episode T: 67 Reward: -153.64184284023025\n",
      "139696887843584\n",
      "Episode Num: 13 Episode T: 114 Reward: -532.1808064223721\n",
      "139696887843584\n",
      "Episode Num: 14 Episode T: 98 Reward: -388.5653024844448\n",
      "139696887843584\n",
      "Episode Num: 15 Episode T: 100 Reward: -510.7437320332252\n",
      "139696887843584\n",
      "Episode Num: 16 Episode T: 96 Reward: -452.27179616927805\n",
      "139696887843584\n",
      "Episode Num: 17 Episode T: 70 Reward: -123.62770535524328\n",
      "139696887843584\n",
      "Episode Num: 18 Episode T: 107 Reward: -606.1809985355617\n",
      "139696887843584\n",
      "Episode Num: 19 Episode T: 80 Reward: -207.10116668154615\n",
      "139696887843584\n",
      "Episode Num: 20 Episode T: 85 Reward: -373.75554370016397\n",
      "139696887843584\n",
      "Episode Num: 21 Episode T: 88 Reward: -336.8827999064955\n",
      "139696887843584\n",
      "Episode Num: 22 Episode T: 116 Reward: -608.2025518182256\n",
      "139696887843584\n",
      "Episode Num: 23 Episode T: 96 Reward: -432.0881848073517\n",
      "139696887843584\n",
      "Episode Num: 24 Episode T: 103 Reward: -450.61366412536984\n",
      "139696887843584\n",
      "Episode Num: 25 Episode T: 87 Reward: -399.34726356512965\n",
      "139696887843584\n",
      "Episode Num: 26 Episode T: 89 Reward: -399.534027774288\n",
      "139696887843584\n",
      "Episode Num: 27 Episode T: 103 Reward: -453.7198059690812\n",
      "139696887843584\n",
      "Episode Num: 28 Episode T: 100 Reward: -507.29476604695236\n",
      "139696887843584\n",
      "Episode Num: 29 Episode T: 88 Reward: -269.08202952884216\n",
      "139696887843584\n",
      "Episode Num: 30 Episode T: 95 Reward: -401.83751034436284\n",
      "139696887843584\n",
      "Episode Num: 31 Episode T: 98 Reward: -500.2579108452199\n",
      "139696887843584\n",
      "Episode Num: 32 Episode T: 102 Reward: -569.2109501704521\n",
      "139696887843584\n",
      "Episode Num: 33 Episode T: 98 Reward: -496.43953210028263\n",
      "139696887843584\n",
      "Episode Num: 34 Episode T: 108 Reward: -657.3369764118356\n",
      "139696887843584\n",
      "Episode Num: 35 Episode T: 72 Reward: -45.13577092427089\n",
      "139696887843584\n",
      "Episode Num: 36 Episode T: 97 Reward: -460.65220017008244\n",
      "139696887843584\n",
      "Episode Num: 37 Episode T: 125 Reward: -694.2886087727888\n",
      "139696887843584\n",
      "Episode Num: 38 Episode T: 89 Reward: -368.09999914243514\n",
      "139696887843584\n",
      "Episode Num: 39 Episode T: 94 Reward: -490.2874589426086\n",
      "139696887843584\n",
      "Episode Num: 40 Episode T: 107 Reward: -577.4389929306361\n",
      "139696887843584\n",
      "Episode Num: 41 Episode T: 91 Reward: -483.6959602082924\n",
      "139696887843584\n",
      "Episode Num: 42 Episode T: 104 Reward: -541.2296066318357\n",
      "139696887843584\n",
      "Episode Num: 43 Episode T: 91 Reward: -430.13266793075024\n",
      "139696887843584\n",
      "Episode Num: 44 Episode T: 101 Reward: -548.5713860495052\n",
      "139696887843584\n",
      "Episode Num: 45 Episode T: 92 Reward: -310.1131541177819\n",
      "139696887843584\n",
      "Episode Num: 46 Episode T: 94 Reward: -510.3377592475654\n",
      "139696887843584\n",
      "Episode Num: 47 Episode T: 99 Reward: -430.8713807454323\n",
      "139696887843584\n",
      "Episode Num: 48 Episode T: 97 Reward: -562.7704928341786\n",
      "139696887843584\n",
      "Episode Num: 49 Episode T: 108 Reward: -602.0998677843882\n",
      "139696887843584\n",
      "Episode Num: 50 Episode T: 108 Reward: -603.4141984504189\n",
      "139696887843584\n",
      "Episode Num: 51 Episode T: 97 Reward: -329.6642411925289\n",
      "139696887843584\n",
      "Episode Num: 52 Episode T: 106 Reward: -508.54893429700815\n",
      "139696887843584\n",
      "Episode Num: 53 Episode T: 96 Reward: -450.04584952561726\n",
      "139696887843584\n",
      "Episode Num: 54 Episode T: 94 Reward: -472.71736107656864\n",
      "139696887843584\n",
      "Episode Num: 55 Episode T: 97 Reward: -530.6875753215204\n",
      "139696887843584\n",
      "Episode Num: 56 Episode T: 99 Reward: -486.4998565811435\n",
      "139696887843584\n",
      "Episode Num: 57 Episode T: 98 Reward: -400.3503233934953\n",
      "139696887843584\n",
      "Episode Num: 58 Episode T: 91 Reward: -390.9914798294802\n",
      "139696887843584\n",
      "Episode Num: 59 Episode T: 95 Reward: -453.74144602860844\n",
      "139696887843584\n",
      "Episode Num: 60 Episode T: 88 Reward: -392.7792541963698\n",
      "139696887843584\n",
      "Episode Num: 61 Episode T: 96 Reward: -504.1909017448894\n",
      "139696887843584\n",
      "Episode Num: 62 Episode T: 98 Reward: -461.4407879340486\n",
      "139696887843584\n",
      "Episode Num: 63 Episode T: 112 Reward: -587.6658195269105\n",
      "139696887843584\n",
      "Episode Num: 64 Episode T: 82 Reward: -323.6780644873361\n",
      "139696887843584\n",
      "Episode Num: 65 Episode T: 99 Reward: -445.54361374161994\n",
      "139696887843584\n",
      "Episode Num: 66 Episode T: 112 Reward: -481.8203124789098\n",
      "139696887843584\n",
      "Episode Num: 67 Episode T: 102 Reward: -598.9688492259847\n",
      "139696887843584\n",
      "Episode Num: 68 Episode T: 94 Reward: -501.0805526350368\n",
      "139696887843584\n",
      "Episode Num: 69 Episode T: 101 Reward: -567.4651296774134\n",
      "139696887843584\n",
      "Episode Num: 70 Episode T: 65 Reward: -112.03707779638263\n",
      "139696887843584\n",
      "Episode Num: 71 Episode T: 97 Reward: -417.1927206470657\n",
      "139696887843584\n",
      "Episode Num: 72 Episode T: 96 Reward: -477.35982197628863\n",
      "139696887843584\n",
      "Episode Num: 73 Episode T: 102 Reward: -565.4593422879068\n",
      "139696887843584\n",
      "Episode Num: 74 Episode T: 97 Reward: -563.8711180256421\n",
      "139696887843584\n",
      "Episode Num: 75 Episode T: 106 Reward: -596.5961714726625\n",
      "139696887843584\n",
      "Episode Num: 76 Episode T: 97 Reward: -500.91104586053194\n",
      "139696887843584\n",
      "Episode Num: 77 Episode T: 74 Reward: -134.60097791257456\n",
      "139696887843584\n",
      "Episode Num: 78 Episode T: 102 Reward: -529.7328617363721\n",
      "139696887843584\n",
      "Episode Num: 79 Episode T: 86 Reward: -335.6903310400579\n",
      "139696887843584\n",
      "Episode Num: 80 Episode T: 108 Reward: -597.509269852416\n",
      "139696887843584\n",
      "Episode Num: 81 Episode T: 106 Reward: -487.32076384218465\n",
      "139696887843584\n",
      "Episode Num: 82 Episode T: 86 Reward: -296.29147251750584\n",
      "139696887843584\n",
      "Episode Num: 83 Episode T: 93 Reward: -372.58972019164776\n",
      "139696887843584\n",
      "Episode Num: 84 Episode T: 101 Reward: -555.4020994091018\n",
      "139696887843584\n",
      "Episode Num: 85 Episode T: 96 Reward: -394.65808181129\n",
      "139696887843584\n",
      "Episode Num: 86 Episode T: 105 Reward: -597.2669701440127\n",
      "139696887843584\n",
      "Episode Num: 87 Episode T: 100 Reward: -408.1297949772532\n",
      "139696887843584\n",
      "Episode Num: 88 Episode T: 101 Reward: -546.7850478431822\n",
      "139696887843584\n",
      "Episode Num: 89 Episode T: 105 Reward: -533.8119715368969\n",
      "139696887843584\n",
      "Episode Num: 90 Episode T: 143 Reward: -567.5394003652196\n",
      "139696887843584\n",
      "Episode Num: 91 Episode T: 54 Reward: 40.948183106707724\n",
      "139696887843584\n",
      "Episode Num: 92 Episode T: 91 Reward: -456.66112968871454\n",
      "139696887843584\n",
      "Episode Num: 93 Episode T: 70 Reward: -36.58337025086206\n",
      "139696887843584\n",
      "Episode Num: 94 Episode T: 98 Reward: -437.85630740016376\n",
      "139696887843584\n",
      "Episode Num: 95 Episode T: 98 Reward: -460.9840833562898\n",
      "139696887843584\n",
      "Episode Num: 96 Episode T: 99 Reward: -390.3909665662798\n",
      "139696887843584\n",
      "Episode Num: 97 Episode T: 92 Reward: -555.9755645895481\n",
      "139696887843584\n",
      "Episode Num: 98 Episode T: 105 Reward: -496.2715582937286\n",
      "139696887843584\n",
      "Episode Num: 99 Episode T: 68 Reward: -19.077083093506356\n",
      "139696887843584\n",
      "Episode Num: 100 Episode T: 92 Reward: -452.91658891486327\n",
      "139696887843584\n",
      "Episode Num: 101 Episode T: 99 Reward: -558.7199738787264\n",
      "139696887843584\n",
      "Episode Num: 102 Episode T: 98 Reward: -547.3225625237258\n",
      "139696887843584\n",
      "Episode Num: 103 Episode T: 98 Reward: -589.2934773153895\n",
      "139696887843584\n",
      "Episode Num: 104 Episode T: 97 Reward: -514.6848542462625\n",
      "139696887843584\n",
      "Episode Num: 105 Episode T: 96 Reward: -502.53584060662376\n",
      "139696887843584\n",
      "Episode Num: 106 Episode T: 60 Reward: 17.288369470179823\n",
      "139696887843584\n",
      "Episode Num: 107 Episode T: 84 Reward: -261.82913201576264\n",
      "139696887843584\n",
      "Episode Num: 108 Episode T: 99 Reward: -535.0948052157511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139696887843584\n",
      "Episode Num: 109 Episode T: 95 Reward: -486.5481670447246\n",
      "139696887843584\n",
      "Episode Num: 110 Episode T: 63 Reward: 0.5272288347895913\n",
      "139696887843584\n",
      "Episode Num: 111 Episode T: 102 Reward: -419.8231893465308\n",
      "139696887843584\n",
      "Episode Num: 112 Episode T: 94 Reward: -513.8868664812738\n",
      "139696887843584\n",
      "Episode Num: 113 Episode T: 94 Reward: -283.2614310294514\n",
      "139696887843584\n",
      "Episode Num: 114 Episode T: 95 Reward: -344.93823454720916\n",
      "139696887843584\n",
      "Episode Num: 115 Episode T: 112 Reward: -647.352067528304\n",
      "139696887843584\n",
      "Episode Num: 116 Episode T: 81 Reward: -266.8906903562229\n",
      "139696887843584\n",
      "Episode Num: 117 Episode T: 109 Reward: -434.9146034573778\n",
      "139696887843584\n",
      "Episode Num: 118 Episode T: 127 Reward: -703.0957249109576\n",
      "139696887843584\n",
      "Episode Num: 119 Episode T: 97 Reward: -584.6468195040234\n",
      "139696887843584\n",
      "Episode Num: 120 Episode T: 94 Reward: -436.42765817946207\n",
      "139696887843584\n",
      "Episode Num: 121 Episode T: 117 Reward: -641.8382231416864\n",
      "139696887843584\n",
      "Episode Num: 122 Episode T: 61 Reward: -23.739635789264387\n",
      "139696887843584\n",
      "Episode Num: 123 Episode T: 100 Reward: -543.6035987745693\n",
      "139696887843584\n",
      "Episode Num: 124 Episode T: 97 Reward: -483.00806858135377\n",
      "139696887843584\n",
      "Episode Num: 125 Episode T: 75 Reward: -211.04022251391396\n",
      "139696887843584\n",
      "Episode Num: 126 Episode T: 95 Reward: -510.18190692621044\n",
      "139696887843584\n",
      "Episode Num: 127 Episode T: 100 Reward: -391.14907124607186\n",
      "139696887843584\n",
      "Episode Num: 128 Episode T: 87 Reward: -291.41184445208256\n",
      "139696887843584\n",
      "Episode Num: 129 Episode T: 96 Reward: -496.30053083278455\n",
      "139696887843584\n",
      "Episode Num: 130 Episode T: 107 Reward: -470.22894598546213\n",
      "139696887843584\n",
      "Episode Num: 131 Episode T: 114 Reward: -519.7178056340108\n",
      "139696887843584\n",
      "Episode Num: 132 Episode T: 121 Reward: -309.56030703375393\n",
      "139696887843584\n",
      "Episode Num: 133 Episode T: 97 Reward: -538.5875807240789\n",
      "139696887843584\n",
      "Episode Num: 134 Episode T: 94 Reward: -297.7691929141505\n",
      "139696887843584\n",
      "Episode Num: 135 Episode T: 96 Reward: -501.56010528652996\n",
      "139696887843584\n",
      "Episode Num: 136 Episode T: 112 Reward: -446.42611649043005\n",
      "139696887843584\n",
      "Episode Num: 137 Episode T: 101 Reward: -568.0549547397577\n",
      "139696887843584\n",
      "Episode Num: 138 Episode T: 133 Reward: -141.8548076412068\n",
      "139696887843584\n",
      "Episode Num: 139 Episode T: 95 Reward: -497.99511143929556\n",
      "139696887843584\n",
      "Episode Num: 140 Episode T: 95 Reward: -493.3798091715362\n",
      "139696887843584\n",
      "Episode Num: 141 Episode T: 99 Reward: -424.80232445918165\n",
      "139696887843584\n",
      "Episode Num: 142 Episode T: 88 Reward: -447.03355187669\n",
      "139696887843584\n",
      "Episode Num: 143 Episode T: 96 Reward: -533.6166531063132\n",
      "139696887843584\n",
      "Episode Num: 144 Episode T: 91 Reward: -477.2556447866824\n",
      "139696887843584\n",
      "Episode Num: 145 Episode T: 66 Reward: -20.33659780794776\n",
      "139696887843584\n",
      "Episode Num: 146 Episode T: 108 Reward: -496.9195011819828\n",
      "139696887843584\n",
      "Episode Num: 147 Episode T: 94 Reward: -463.6380488295248\n",
      "139696887843584\n",
      "Episode Num: 148 Episode T: 97 Reward: -448.9851472830117\n",
      "139696887843584\n",
      "Episode Num: 149 Episode T: 91 Reward: -502.5676388616195\n",
      "139696887843584\n",
      "Episode Num: 150 Episode T: 77 Reward: -166.42822670866005\n",
      "139696887843584\n",
      "Episode Num: 151 Episode T: 103 Reward: -498.7702806012925\n",
      "139696887843584\n",
      "Episode Num: 152 Episode T: 100 Reward: -585.1136633959909\n",
      "139696887843584\n",
      "Episode Num: 153 Episode T: 96 Reward: -543.3178325346407\n",
      "139696887843584\n",
      "Episode Num: 154 Episode T: 98 Reward: -614.1519086385066\n",
      "139696887843584\n",
      "Episode Num: 155 Episode T: 112 Reward: -498.8824650341163\n",
      "139696887843584\n",
      "Episode Num: 156 Episode T: 62 Reward: -13.17419977352204\n",
      "139696887843584\n",
      "Episode Num: 157 Episode T: 103 Reward: -441.77322468781534\n",
      "139696887843584\n",
      "Episode Num: 158 Episode T: 94 Reward: -559.9250750635467\n",
      "139696887843584\n",
      "Episode Num: 159 Episode T: 95 Reward: -444.3213142133801\n",
      "139696887843584\n",
      "Episode Num: 160 Episode T: 109 Reward: -518.4951013670914\n",
      "139696887843584\n",
      "Episode Num: 161 Episode T: 99 Reward: -530.0690120906163\n",
      "139696887843584\n",
      "Episode Num: 162 Episode T: 102 Reward: -661.2297609264951\n",
      "139696887843584\n",
      "Episode Num: 163 Episode T: 103 Reward: -480.35735374095225\n",
      "139696887843584\n",
      "Episode Num: 164 Episode T: 90 Reward: -441.9643116355356\n",
      "139696887843584\n",
      "Episode Num: 165 Episode T: 103 Reward: -539.2274847459881\n",
      "139696887843584\n",
      "Episode Num: 166 Episode T: 94 Reward: -475.86977625623365\n",
      "139696887843584\n",
      "Episode Num: 167 Episode T: 92 Reward: -480.7042633928351\n",
      "139696887843584\n",
      "Episode Num: 168 Episode T: 90 Reward: -368.6413576570372\n",
      "139696887843584\n",
      "Episode Num: 169 Episode T: 99 Reward: -497.20389909139305\n",
      "139696887843584\n",
      "Episode Num: 170 Episode T: 109 Reward: -594.7829914383208\n",
      "139696887843584\n",
      "Episode Num: 171 Episode T: 108 Reward: -426.30341750355853\n",
      "139696887843584\n",
      "Episode Num: 172 Episode T: 119 Reward: -583.3974389272242\n",
      "139696887843584\n",
      "Episode Num: 173 Episode T: 86 Reward: -396.04395087508647\n",
      "139696887843584\n",
      "Episode Num: 174 Episode T: 98 Reward: -515.6991880086691\n",
      "139696887843584\n",
      "Episode Num: 175 Episode T: 85 Reward: -381.6802273835042\n",
      "139696887843584\n",
      "Episode Num: 176 Episode T: 69 Reward: -234.2619643917438\n",
      "139696887843584\n",
      "Episode Num: 177 Episode T: 119 Reward: -320.66742888192675\n",
      "139696887843584\n",
      "Episode Num: 178 Episode T: 90 Reward: -429.2979292060059\n",
      "139696887843584\n",
      "Episode Num: 179 Episode T: 91 Reward: -238.71751099823405\n",
      "139696887843584\n",
      "Episode Num: 180 Episode T: 85 Reward: -230.489743867608\n",
      "139696887843584\n",
      "Episode Num: 181 Episode T: 96 Reward: -478.08606673104254\n",
      "139696887843584\n",
      "Episode Num: 182 Episode T: 97 Reward: -492.17845735763683\n",
      "139696887843584\n",
      "Episode Num: 183 Episode T: 95 Reward: -462.9497152233101\n",
      "139696887843584\n",
      "Episode Num: 184 Episode T: 104 Reward: -574.998679225834\n",
      "139696887843584\n",
      "Episode Num: 185 Episode T: 132 Reward: -614.9089072771679\n",
      "139696887843584\n",
      "Episode Num: 186 Episode T: 132 Reward: -602.9622223164473\n",
      "139696887843584\n",
      "Episode Num: 187 Episode T: 98 Reward: -549.0814587528174\n",
      "139696887843584\n",
      "Episode Num: 188 Episode T: 69 Reward: -150.76635858078564\n",
      "139696887843584\n",
      "Episode Num: 189 Episode T: 102 Reward: -469.729140004684\n",
      "139696887843584\n",
      "Episode Num: 190 Episode T: 94 Reward: -379.65754153972614\n",
      "139696887843584\n",
      "Episode Num: 191 Episode T: 90 Reward: -359.0505744204843\n",
      "139696887843584\n",
      "Episode Num: 192 Episode T: 97 Reward: -440.6187840279104\n",
      "139696887843584\n",
      "Episode Num: 193 Episode T: 97 Reward: -540.2127456198825\n",
      "139696887843584\n",
      "Episode Num: 194 Episode T: 105 Reward: -475.8738317907899\n",
      "139696887843584\n",
      "Episode Num: 195 Episode T: 97 Reward: -156.69835753480476\n",
      "139696887843584\n",
      "Episode Num: 196 Episode T: 95 Reward: -523.9780924671555\n",
      "139696887843584\n",
      "Episode Num: 197 Episode T: 95 Reward: -502.3476560706929\n",
      "139696887843584\n",
      "Episode Num: 198 Episode T: 85 Reward: -402.04834693593483\n",
      "139696887843584\n",
      "Episode Num: 199 Episode T: 69 Reward: -199.88376375683242\n",
      "139696887843584\n",
      "Episode Num: 200 Episode T: 76 Reward: -398.15117287732346\n",
      "139696887843584\n",
      "Episode Num: 201 Episode T: 102 Reward: -623.9280059461643\n",
      "139696887843584\n",
      "Episode Num: 202 Episode T: 104 Reward: -647.450706252533\n",
      "139696887843584\n",
      "Episode Num: 203 Episode T: 93 Reward: -418.2617030076663\n",
      "139696887843584\n",
      "Episode Num: 204 Episode T: 103 Reward: -578.7552929988396\n",
      "139696887843584\n",
      "Episode Num: 205 Episode T: 93 Reward: -485.29420147515486\n",
      "139696887843584\n",
      "Episode Num: 206 Episode T: 62 Reward: -17.164440610954863\n",
      "139696887843584\n",
      "Episode Num: 207 Episode T: 96 Reward: -551.949918223769\n",
      "139696887843584\n",
      "Episode Num: 208 Episode T: 86 Reward: -298.06004944363315\n",
      "139696887843584\n",
      "Episode Num: 209 Episode T: 94 Reward: -466.9324974977944\n",
      "139696887843584\n",
      "Episode Num: 210 Episode T: 107 Reward: -482.2970466646714\n",
      "139696887843584\n",
      "Episode Num: 211 Episode T: 100 Reward: -595.1045594341505\n",
      "139696887843584\n",
      "Episode Num: 212 Episode T: 102 Reward: -569.5116690990603\n",
      "139696887843584\n",
      "Episode Num: 213 Episode T: 107 Reward: -464.74048105237557\n",
      "139696887843584\n",
      "Episode Num: 214 Episode T: 101 Reward: -555.8938928843103\n",
      "139696887843584\n",
      "Episode Num: 215 Episode T: 109 Reward: -556.7771268802821\n",
      "139696887843584\n",
      "Episode Num: 216 Episode T: 60 Reward: 29.883636047268663\n",
      "139696887843584\n",
      "Episode Num: 217 Episode T: 113 Reward: -549.2165311444928\n",
      "139696887843584\n",
      "Episode Num: 218 Episode T: 87 Reward: -354.8342465173598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139696887843584\n",
      "Episode Num: 219 Episode T: 93 Reward: -458.28818640716577\n",
      "139696887843584\n",
      "Episode Num: 220 Episode T: 95 Reward: -534.6693551089006\n",
      "139696887843584\n",
      "Episode Num: 221 Episode T: 101 Reward: -501.4252968945872\n",
      "139696887843584\n",
      "Episode Num: 222 Episode T: 105 Reward: -400.46371709197496\n",
      "139696887843584\n",
      "Episode Num: 223 Episode T: 94 Reward: -451.28436141506506\n",
      "139696887843584\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function Manager_integrate> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/opensim/simbody.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m  17704\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 17705\u001b[0;31m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  17706\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fc417e3de4dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrollout_random_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-51ddb4cbb0de>\u001b[0m in \u001b[0;36mrollout_random_episode\u001b[0;34m(episode_num)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Perform action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_env_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0menv_step_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# if not done:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nips-2018-ai-for-prosthetics/environment/prosthetics_env_with_history.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProstheticsEnvWithHistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         self.append_history({\n\u001b[1;32m     29\u001b[0m             \u001b[0;34m'episode_uuid'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/osim/env/osim.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, project)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_state_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mosim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactuate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mosim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/osim/env/osim.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# Integrate till the new endtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mistep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/opensim/simulation.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, finalTime)\u001b[0m\n\u001b[1;32m  38432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  38433\u001b[0m         \"\"\"\n\u001b[0;32m> 38434\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_simulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mManager_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  38435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  38436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function Manager_integrate> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "# Unfortunately, we can't use multiprocessing with opensim.  \n",
    "# Even if we init separate environments for each thread, it \n",
    "# seems that they might share an opensim instance.... :facepalm:\n",
    "\n",
    "# eps = [[i] for i in range(500)]\n",
    "# pool = multiprocessing.Pool() # Defaults to os.cpu_count() for number of threads in the pool.\n",
    "# rval = pool.starmap_async(rollout_random_episode, eps)#, callback=callback)\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "for i in range(500):\n",
    "    rollout_random_episode(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosthetics",
   "language": "python",
   "name": "prosthetics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
