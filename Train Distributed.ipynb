{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import uuid\n",
    "import datetime\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from models.td3 import TD3\n",
    "# from osim.env import ProstheticsEnv\n",
    "from environment.prosthetics_env_with_history import ProstheticsEnvWithHistory\n",
    "from environment.observations import prepare_model_observation, env_obs_history_to_model_obs\n",
    "# from environment.actions import prepare_env_action, reset_frameskip\n",
    "from environment.rewards import env_obs_to_custom_reward\n",
    "from distributed.database import persist_timesteps, persist_event, get_total_timesteps, clear_clients_for_thread\n",
    "from distributed.db_history_sampler import DatabaseHistorySampler\n",
    "from distributed.s3_checkpoints import load_s3_model_checkpoint, save_s3_model_checkpoint\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"env\": {\n",
      "        \"integrator_accuracy\": 0.002\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"architecture\": \"TD3\"\n",
      "    },\n",
      "    \"rollout\": {\n",
      "        \"#\": \"Frameskip will be applied for random durations between 0 and `frameskip` timesteps.\",\n",
      "        \"max_episode_steps\": 600,\n",
      "        \"expl_noise\": 0.25,\n",
      "        \"frameskip\": 5\n",
      "    },\n",
      "    \"distributed\": {\n",
      "        \"policy_weights_dir_s3\": \"s3://colllin-nips-2018-prosthetics/checkpoints/\",\n",
      "        \"policy_weights_basename\": \"checkpoint_TD3\",\n",
      "        \"#\": \"How often (episodes) we download model weights during rollout.\",\n",
      "        \"rollout_refresh_model_freq\": 5\n",
      "    },\n",
      "    \"training\": {\n",
      "        \"#\": \"Frequency of delayed policy updates\",\n",
      "        \"eval_freq\": 2500,\n",
      "        \"batch_size\": 100,\n",
      "        \"discount\": 0.99,\n",
      "        \"tau\": 0.005,\n",
      "        \"policy_noise\": 0.2,\n",
      "        \"noise_clip\": 0.5,\n",
      "        \"policy_freq\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('config_distributed.json', 'r') as f:\n",
    "    CONFIG = json.load(f)\n",
    "print(json.dumps(CONFIG, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Policy, Download & load latest weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1260, 19, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dim = env.observation_space.shape[0]\n",
    "env = ProstheticsEnvWithHistory(visualize=False, integrator_accuracy=CONFIG['env']['integrator_accuracy'])\n",
    "env.reset()\n",
    "state_dim = prepare_model_observation(env).shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = int(env.action_space.high[0])\n",
    "del env\n",
    "state_dim, action_dim, max_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy checkpoints from s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading policy checkpoints from {CONFIG['distributed']['policy_weights_dir_s3']}{CONFIG['distributed']['policy_weights_basename']}*\")\n",
    "load_s3_model_checkpoint(\n",
    "    policy, \n",
    "    s3_dir=CONFIG['distributed']['policy_weights_dir_s3'],\n",
    "    basename=CONFIG['distributed']['policy_weights_basename'],\n",
    "    map_location='cpu'\n",
    ")\n",
    "persist_event('train_load_latest_checkpoint', f'Loaded policy checkpoint from {CONFIG[\"distributed\"][\"policy_weights_dir_s3\"]}{CONFIG[\"distributed\"][\"policy_weights_basename\"]}*')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Hacking (Custom \"done\" criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def should_abort_episode(env_obs, custom_rewards=None, verbose=False):\n",
    "# #     print((np.array(env_obs['body_pos_rot']['torso'])*180/math.pi > 60).any())\n",
    "# #     if env_obs['body_pos_rot']['torso'][2] < -0.2:\n",
    "# #         return True\n",
    "#     rewards = custom_rewards if custom_rewards != None else env_obs_to_custom_reward(env_obs)\n",
    "#     # print(f'Custom reward: {sum(rewards.values())}')\n",
    "#     if (env_obs['body_pos']['head'][0] - env_obs['body_pos']['pelvis'][0]) < -.2:\n",
    "#         if verbose: print(f'Aborting episode due to head being > .2m behind the pelvis ({env_obs[\"body_pos\"][\"head\"][0] - env_obs[\"body_pos\"][\"pelvis\"][0]}).')\n",
    "#         return True\n",
    "#     if np.fabs(env_obs['body_pos']['head'][2]) > 0.5:\n",
    "#         if verbose: print(f'Aborting episode due to head being > 0.5m away from centerline ({env_obs[\"body_pos\"][\"head\"][2]}).')\n",
    "#         return True\n",
    "#     if sum(rewards.values()) < -10:\n",
    "#         if verbose:\n",
    "#             print(f'Aborting episode due to custom reward < -10 ({sum(rewards.values())}):')\n",
    "#             for k,v in rewards.items():\n",
    "#                 if v < 0:\n",
    "#                     print(f'  reward `{k}` = {v}')\n",
    "#         return True\n",
    "#     return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_sampler = DatabaseHistorySampler(\n",
    "    env_obs_history_to_model_obs_fn=env_obs_history_to_model_obs, \n",
    "    n_obs_history=3,\n",
    "#     env_obs_custom_reward_fn=lambda obs: sum(env_obs_to_custom_reward(obs).values()),\n",
    "#     env_obs_custom_done_fn=should_abort_episode,\n",
    ")\n",
    "\n",
    "def load_batch(fake_batch):\n",
    "    return history_sampler.sample(len(fake_batch))\n",
    "\n",
    "fake_dataset_len = CONFIG['training']['eval_freq'] * CONFIG['training']['batch_size']\n",
    "fake_dataset = list(range(int(fake_dataset_len)))\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    fake_dataset,\n",
    "    batch_size=CONFIG['training']['batch_size'], \n",
    "#     shuffle=False, \n",
    "#     sampler=None, \n",
    "#     batch_sampler=None, \n",
    "    num_workers=6, \n",
    "    collate_fn=load_batch, \n",
    "    pin_memory=True, \n",
    "    drop_last=True, \n",
    "#     timeout=0, \n",
    "    worker_init_fn=lambda instance_id: clear_clients_for_thread()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [06:57<00:00,  5.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T21638_2018-10-19T19:27:53.108049*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:49<00:00,  9.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T21638_2018-10-19T19:33:44.266989*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:53<00:00,  8.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T21638_2018-10-19T19:39:39.434553*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:55<00:00,  7.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T21638_2018-10-19T19:45:36.113071*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:50<00:00,  7.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T21638_2018-10-19T19:51:28.709823*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:50<00:00,  7.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T21638_2018-10-19T19:57:20.749984*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:53<00:00,  7.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T21638_2018-10-19T20:03:16.300675*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:52<00:00,  7.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T33334_2018-10-19T20:09:10.137125*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:52<00:00,  7.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T47909_2018-10-19T20:15:04.372928*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:55<00:00,  7.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T51472_2018-10-19T20:21:01.083062*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model: 100%|██████████| 2500/2500 [05:52<00:00,  7.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3*\n",
      "SAving policy checkpoints to s3://colllin-nips-2018-prosthetics/checkpoints/checkpoint_TD3_T61332_2018-10-19T20:26:54.947192*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train model:  42%|████▏     | 1048/2500 [02:27<03:56,  6.15batch/s]"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Train for `eval_freq` batches:\n",
    "    if CONFIG['model']['architecture'] == \"TD3\":\n",
    "        policy.train(\n",
    "            dataloader,\n",
    "            CONFIG['training']['discount'], \n",
    "            CONFIG['training']['tau'], \n",
    "            CONFIG['training']['policy_noise'], \n",
    "            CONFIG['training']['noise_clip'], \n",
    "            CONFIG['training']['policy_freq'],\n",
    "        )\n",
    "    else: \n",
    "        policy.train(\n",
    "            history_sampler,#replay_buffer, \n",
    "            int(CONFIG['training']['eval_freq']),\n",
    "            CONFIG['training']['batch_size'], \n",
    "            CONFIG['training']['discount'], \n",
    "            CONFIG['training']['tau']\n",
    "        )\n",
    "    persist_event('train_epoch_completed', f'Trained policy for {len(dataloader)} batches of {dataloader.batch_size}')\n",
    "      \n",
    "    # Upload policy weights to S3, to be picked up by instances running the Rollout Distributed process.\n",
    "    print(f\"SAving policy checkpoints to {CONFIG['distributed']['policy_weights_dir_s3']}{CONFIG['distributed']['policy_weights_basename']}*\")\n",
    "    save_s3_model_checkpoint(\n",
    "        policy, \n",
    "        s3_dir=CONFIG['distributed']['policy_weights_dir_s3'],\n",
    "        basename=CONFIG['distributed']['policy_weights_basename'],\n",
    "    )\n",
    "    persist_event('train_update_s3_checkpoint', f'Uploaded policy checkpoint to {CONFIG[\"distributed\"][\"policy_weights_dir_s3\"]}{CONFIG[\"distributed\"][\"policy_weights_basename\"]}*')\n",
    "    \n",
    "    # Also upload policy weights under unique name as a historical checkpoint.\n",
    "    total_timesteps = get_total_timesteps()\n",
    "    evalname = f\"{CONFIG['distributed']['policy_weights_basename']}_T{total_timesteps}_{datetime.datetime.now().isoformat()}\"\n",
    "    print(f\"SAving policy checkpoints to {CONFIG['distributed']['policy_weights_dir_s3']}{evalname}*\")\n",
    "    save_s3_model_checkpoint(\n",
    "        policy, \n",
    "        s3_dir=CONFIG['distributed']['policy_weights_dir_s3'],\n",
    "        basename=evalname,\n",
    "    )\n",
    "    persist_event('train_save_historical_checkpoint', f'Uploaded policy checkpoint to {CONFIG[\"distributed\"][\"policy_weights_dir_s3\"]}{evalname}*')\n",
    "\n",
    "    # Run Evaluation script\n",
    "    # evaldir = str(uuid.uuid4())\n",
    "    # print(f\"SAving policy checkpoints to {evaldir}/{evalname}*\")\n",
    "    # os.makedirs(evaldir, exist_ok=True)\n",
    "    # policy.save(evaldir, evalname)\n",
    "    # evalcmd = f\"CHECKPOINT_DIR={evaldir} CHECKPOINT_NAME={evalname} pipenv run python evaluate_policy.py\"\n",
    "    # print(f\"Launching evaluation script with cmd: `{evalcmd}`\")\n",
    "    # subprocess.Popen(evalcmd, shell=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosthetics",
   "language": "python",
   "name": "prosthetics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
