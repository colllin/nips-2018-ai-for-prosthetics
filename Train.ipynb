{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osim.env import ProstheticsEnv\n",
    "from td3 import TD3\n",
    "from replay_buffer import ReplayBuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"env\": {\n",
    "        \"integrator_accuracy\": 1e-2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"architecture\": \"TD3\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"start_timesteps\": 1e4, # How many time steps purely random policy is run for\n",
    "        \"eval_freq\": 5e3, # How often (time steps) we evaluate\n",
    "        \"max_timesteps\": 1e6, # Max time steps to train for\n",
    "        \"max_episode_steps\": 300, # Max number of steps to run for a single episode\n",
    "        \"expl_noise\": 0.1, # Std of Gaussian exploration noise\n",
    "        \"batch_size\": 100, # Batch size for both actor and critic\n",
    "        \"discount\": 0.99, # Discount factor\n",
    "        \"tau\": 0.005, # Target network update rate\n",
    "        \"policy_noise\": 0.2, # Noise added to target policy during critic update\n",
    "        \"noise_clip\": 0.5, # Range to clip target policy noise\n",
    "        \"policy_freq\": 2, # Frequency of delayed policy updates\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path('.')\n",
    "LOGS_DIR = OUTPUT_DIR/'logs'\n",
    "CHECKPOINTS_DIR = OUTPUT_DIR/'checkpoints'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Hacking\n",
    "\n",
    "- Rewrite all joint_pos, body_pos to be relative to mass_center_pos\n",
    "- Subtract mass_center_vel and mass_center_acc from joint_vel, body_vel, joint_acc, body_acc?\n",
    "- Either compute jounce/snap, or pass multiple timesteps, or just pass acceleration from past 3 timesteps?\n",
    "\n",
    "Initial Env Observation:\n",
    "```\n",
    "{\n",
    "    'joint_pos': {\n",
    "        'ground_pelvis': [0.0, 0.0, 0.0, 0.0, 0.94, 0.0],\n",
    "        'hip_r': [0.0, 0.0, 0.0],\n",
    "        'knee_r': [0.0],\n",
    "        'ankle_r': [0.0],\n",
    "        'hip_l': [0.0, 0.0, 0.0],\n",
    "        'knee_l': [0.0],\n",
    "        'ankle_l': [0.0],\n",
    "        'subtalar_l': [],\n",
    "        'mtp_l': [],\n",
    "        'back': [-0.0872665],\n",
    "        'back_0': []\n",
    "    },\n",
    "    'joint_vel': {\n",
    "        'ground_pelvis': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        'hip_r': [0.0, 0.0, 0.0],\n",
    "        'knee_r': [0.0],\n",
    "        'ankle_r': [0.0],\n",
    "        'hip_l': [0.0, 0.0, 0.0],\n",
    "        'knee_l': [0.0],\n",
    "        'ankle_l': [0.0],\n",
    "        'subtalar_l': [],\n",
    "        'mtp_l': [],\n",
    "        'back': [0.0],\n",
    "        'back_0': []\n",
    "    },\n",
    "    'joint_acc': {\n",
    "        'ground_pelvis': [34.07237489546962, 3.219284560937942, 0.021285761200362296, 13.997154494145377, 0.8655672359505977, -0.6156967622871027],\n",
    "        'hip_r': [-194.74323476194263, -4.441803696780512, 1.5931700403370996e-14],\n",
    "        'knee_r': [305.46152469620915],\n",
    "        'ankle_r': [9636.363025843913],\n",
    "        'hip_l': [-208.86020665024324, 3.5702556374966354, -4.2521541843143495e-14],\n",
    "        'knee_l': [399.3192427973721],\n",
    "        'ankle_l': [809.4478175113452],\n",
    "        'subtalar_l': [],\n",
    "        'mtp_l': [],\n",
    "        'back': [-2.3092638912203256e-14],\n",
    "        'back_0': []\n",
    "    },\n",
    "    'body_pos': {\n",
    "        'pelvis': [0.0, 0.94, 0.0],\n",
    "        'femur_r': [-0.0707, 0.8738999999999999, 0.0835],\n",
    "        'pros_tibia_r': [-0.07519985651753601, 0.47807930355164957, 0.0835],\n",
    "        'pros_foot_r': [-0.07519985651753601, 0.04807930355164958, 0.0835],\n",
    "        'femur_l': [-0.0707, 0.8738999999999999, -0.0835],\n",
    "        'tibia_l': [-0.07519985651753601, 0.47807930355164957, -0.0835],\n",
    "        'talus_l': [-0.07519985651753601, 0.04807930355164958, -0.0835],\n",
    "        'calcn_l': [-0.123969856517536, 0.006129303551649576, -0.09142],\n",
    "        'toes_l': [0.05483014348246398, 0.004129303551649576, -0.0925],\n",
    "        'torso': [-0.1007, 1.0214999999999999, 0.0],\n",
    "        'head': [-0.052764320996907754, 1.5694070821576522, 0.0]\n",
    "    },\n",
    "    'body_vel': {\n",
    "        'pelvis': [0.0, 0.0, 0.0],\n",
    "        'femur_r': [0.0, 0.0, 0.0],\n",
    "        'pros_tibia_r': [0.0, 0.0, 0.0],\n",
    "        'pros_foot_r': [0.0, 0.0, 0.0],\n",
    "        'femur_l': [0.0, 0.0, 0.0],\n",
    "        'tibia_l': [0.0, 0.0, 0.0],\n",
    "        'talus_l': [0.0, 0.0, 0.0],\n",
    "        'calcn_l': [0.0, 0.0, 0.0],\n",
    "        'toes_l': [0.0, 0.0, 0.0],\n",
    "        'torso': [0.0, 0.0, 0.0],\n",
    "        'head': [0.0, 0.0, 0.0]\n",
    "    },\n",
    "    'body_acc': {\n",
    "        'pelvis': [13.997154494145377, 0.8655672359505977, -0.6156967622871027],\n",
    "        'femur_r': [16.25111583579615, -1.812159929997423, -0.826986568448235],\n",
    "        'pros_tibia_r': [-49.070641675940735, 0.12065763836075294, -0.34299240980632545],\n",
    "        'pros_foot_r': [13.18934420084581, 0.12065763836075294, 0.18269081860597952],\n",
    "        'femur_l': [16.24756111367569, -1.2745394083207864, -0.826986568448235],\n",
    "        'tibia_l': [-55.19198970892064, 1.093538716356541, -0.6879691696202773],\n",
    "        'talus_l': [41.356517039396714, 1.093538716356541, -0.537051606700039],\n",
    "        'calcn_l': [84.73177709400595, -49.336407951145645, -0.5212902634646581],\n",
    "        'toes_l': [86.79971256249173, 135.5386990655368, -0.5243942154141731],\n",
    "        'torso': [11.220255940164602, -2.565520916023193, -0.35118159441778396],\n",
    "        'head': [-7.448239570993795, -0.9322384901609437, 1.4116668685846663]\n",
    "    },\n",
    "    'body_pos_rot': {\n",
    "        'pelvis': [-0.0, 0.0, -0.0],\n",
    "        'femur_r': [-0.0, 0.0, -0.0],\n",
    "        'pros_tibia_r': [-0.0, 0.0, -0.0],\n",
    "        'pros_foot_r': [-0.0, 0.0, -0.0],\n",
    "        'femur_l': [-0.0, 0.0, -0.0],\n",
    "        'tibia_l': [-0.0, 0.0, -0.0],\n",
    "        'talus_l': [-0.0, 0.0, -0.0],\n",
    "        'calcn_l': [-0.0, 0.0, -0.0],\n",
    "        'toes_l': [-0.0, 0.0, -0.0],\n",
    "        'torso': [-0.0, 0.0, -0.0872665],\n",
    "        'head': [-0.0, 0.0, -0.0872665]\n",
    "    },\n",
    "    'body_vel_rot': {\n",
    "        'pelvis': [0.0, 0.0, 0.0],\n",
    "        'femur_r': [0.0, 0.0, 0.0],\n",
    "        'pros_tibia_r': [0.0, 0.0, 0.0],\n",
    "        'pros_foot_r': [0.0, 0.0, 0.0],\n",
    "        'femur_l': [0.0, 0.0, 0.0],\n",
    "        'tibia_l': [0.0, 0.0, 0.0],\n",
    "        'talus_l': [0.0, 0.0, 0.0],\n",
    "        'calcn_l': [0.0, 0.0, 0.0],\n",
    "        'toes_l': [0.0, 0.0, 0.0],\n",
    "        'torso': [0.0, 0.0, 0.0],\n",
    "        'head': [0.0, 0.0, 0.0]\n",
    "    },\n",
    "    'body_acc_rot': {\n",
    "        'pelvis': [3.219284560937942, 0.021285761200362296, 34.07237489546962],\n",
    "        'femur_r': [-1.2225191358425698, 0.021285761200378228, -160.670859866473],\n",
    "        'pros_tibia_r': [-1.2225191358425698, 0.021285761200378228, 144.79066482973616],\n",
    "        'pros_foot_r': [-1.2225191358425698, 0.021285761200378228, 9781.15369067365],\n",
    "        'femur_l': [-0.35097107655869353, 0.021285761200404818, -174.7878317547736],\n",
    "        'tibia_l': [-0.35097107655869353, 0.021285761200404818, 224.5314110425985],\n",
    "        'talus_l': [-0.35097107655869353, 0.021285761200404818, 1033.9792285539438],\n",
    "        'calcn_l': [-0.35097107655869353, 0.021285761200404818, 1033.9792285539438],\n",
    "        'toes_l': [-0.35097107655869353, 0.021285761200404818, 1033.9792285539438],\n",
    "        'torso': [3.219284560937942, 0.021285761200362296, 34.0723748954696],\n",
    "        'head': [3.219284560937942, 0.021285761200362296, 34.0723748954696]\n",
    "    },\n",
    "    'forces': {\n",
    "        'abd_r': [219.6613927253564],\n",
    "        'add_r': [144.87433100305103],\n",
    "        'hamstrings_r': [194.30030504346755],\n",
    "        'bifemsh_r': [42.728811234363775],\n",
    "        'glut_max_r': [171.7873509605573],\n",
    "        'iliopsoas_r': [158.01207984383657],\n",
    "        'rect_fem_r': [99.0329705435046],\n",
    "        'vasti_r': [436.79388413623326],\n",
    "        'abd_l': [219.6613927253564],\n",
    "        'add_l': [144.87433100305103],\n",
    "        'hamstrings_l': [194.30030504346755],\n",
    "        'bifemsh_l': [42.728811234363775],\n",
    "        'glut_max_l': [171.7873509605573],\n",
    "        'iliopsoas_l': [158.01207984383657],\n",
    "        'rect_fem_l': [99.0329705435046],\n",
    "        'vasti_l': [436.79388413623326],\n",
    "        'gastroc_l': [273.0178325689043],\n",
    "        'soleus_l': [370.0059951709156],\n",
    "        'tib_ant_l': [104.05059952034294],\n",
    "        'ankleSpring': [-0.0],\n",
    "        'pros_foot_r_0': [-1.3573320551122304e-12, -388.7553514927188, 0.0, 32.46107184964201, -1.1333722660187127e-13, 3.726164264482634, 1.3573320551122304e-12, 388.7553514927188, 0.0, 0.0, 0.0, 25.50818238819416, 1.3573320551122304e-12, 388.7553514927188, 0.0, 0.0, 0.0, 25.50818238819416],\n",
    "        'foot_l': [-1.7615592933859115e-12, -504.53063397142085, 0.0, -46.45915575255036, 1.622112753284362e-13, -4.943148065393853, 6.786660275561278e-13, 194.37767574636297, 0.0, 0.0, 0.0, 5.831330272390875, 1.0828932658297836e-12, 310.1529582250579, 0.0, 0.0, 0.0, 6.203059164501164, 1.0828932658297836e-12, 310.1529582250579, 0.0, 0.0, 0.0, 6.203059164501164],\n",
    "        'HipLimit_r': [0.0, 0.0],\n",
    "        'HipLimit_l': [0.0, 0.0],\n",
    "        'KneeLimit_r': [-0.0, 0.0],\n",
    "        'KneeLimit_l': [-0.0, 0.0],\n",
    "        'AnkleLimit_r': [0.0, 0.0],\n",
    "        'AnkleLimit_l': [0.0, 0.0],\n",
    "        'HipAddLimit_r': [0.0, 0.0],\n",
    "        'HipAddLimit_l': [0.0, 0.0]\n",
    "    },\n",
    "    'muscles': {\n",
    "        'abd_r': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.07752306863700548,\n",
    "            'fiber_velocity': 1.1700156898117815e-13,\n",
    "            'fiber_force': 219.6613927253564\n",
    "        },\n",
    "        'add_r': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.05526137592854144,\n",
    "            'fiber_velocity': 5.531257930905764e-11,\n",
    "            'fiber_force': 146.25768888087705\n",
    "        },\n",
    "        'hamstrings_r': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.06355896214015513,\n",
    "            'fiber_velocity': 2.1056261406660054e-14,\n",
    "            'fiber_force': 202.45627069225887\n",
    "        },\n",
    "        'bifemsh_r': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.13434264681417835,\n",
    "            'fiber_velocity': 9.542198984660805e-17,\n",
    "            'fiber_force': 45.09919197278584\n",
    "        },\n",
    "        'glut_max_r': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.16084824667171801,\n",
    "            'fiber_velocity': 1.0181982508865008e-12,\n",
    "            'fiber_force': 171.7873509605573\n",
    "        },\n",
    "        'iliopsoas_r': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.13005768603600326,\n",
    "            'fiber_velocity': 3.347183497651294e-11,\n",
    "            'fiber_force': 159.26525950285387\n",
    "        },\n",
    "        'rect_fem_r': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.06027044615978444,\n",
    "            'fiber_velocity': 2.2362024955832438e-15,\n",
    "            'fiber_force': 99.63652479982161\n",
    "        },\n",
    "        'vasti_r': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.07890756873654925,\n",
    "            'fiber_velocity': 6.168233828156989e-15,\n",
    "            'fiber_force': 437.7385693769557\n",
    "        },\n",
    "        'abd_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.07752306863700548,\n",
    "            'fiber_velocity': 1.1700156898117815e-13,\n",
    "            'fiber_force': 219.6613927253564\n",
    "        },\n",
    "        'add_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.05526137592854144,\n",
    "            'fiber_velocity': 5.531257930905764e-11,\n",
    "            'fiber_force': 146.25768888087705\n",
    "        },\n",
    "        'hamstrings_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.06355896214015513,\n",
    "            'fiber_velocity': 2.1056261406660054e-14,\n",
    "            'fiber_force': 202.45627069225887\n",
    "        },\n",
    "        'bifemsh_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.13434264681417835,\n",
    "            'fiber_velocity': 9.542198984660805e-17,\n",
    "            'fiber_force': 45.09919197278584\n",
    "        },\n",
    "        'glut_max_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.16084824667171801,\n",
    "            'fiber_velocity': 1.0181982508865008e-12,\n",
    "            'fiber_force': 171.7873509605573\n",
    "        },\n",
    "        'iliopsoas_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.13005768603600326,\n",
    "            'fiber_velocity': 3.347183497651294e-11,\n",
    "            'fiber_force': 159.26525950285387\n",
    "        },\n",
    "        'rect_fem_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.06027044615978444,\n",
    "            'fiber_velocity': 2.2362024955832438e-15,\n",
    "            'fiber_force': 99.63652479982161\n",
    "        },\n",
    "        'vasti_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.07890756873654925,\n",
    "            'fiber_velocity': 6.168233828156989e-15,\n",
    "            'fiber_force': 437.7385693769557\n",
    "        },\n",
    "        'gastroc_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.05720257668702345,\n",
    "            'fiber_velocity': 5.718949639274886e-14,\n",
    "            'fiber_force': 282.79456495087237\n",
    "        },\n",
    "        'soleus_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.04494814124106819,\n",
    "            'fiber_velocity': 3.4120643802478774e-10,\n",
    "            'fiber_force': 406.4161372187392\n",
    "        },\n",
    "        'tib_ant_l': {\n",
    "            'activation': 0.05,\n",
    "            'fiber_length': 0.06288000983990409,\n",
    "            'fiber_velocity': 8.525642140971546e-14,\n",
    "            'fiber_force': 104.5158690359632\n",
    "        }\n",
    "    },\n",
    "    'markers': {},\n",
    "    'misc': {\n",
    "        'mass_center_pos': [-0.08466565561225976, 0.9952730567231536, -0.003576087446004414],\n",
    "        'mass_center_vel': [0.0, 0.0, 0.0],\n",
    "        'mass_center_acc': [4.4008799039045146e-14, 2.570832209970726, -1.0237645330147003e-15]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/stanfordnmbl/osim-rl/blob/master/osim/env/osim.py#L452\n",
    "history = {}\n",
    "\n",
    "def add_partial_obs_to_history(model_obs):\n",
    "    if history['t'] == model_obs:\n",
    "        return\n",
    "    \n",
    "    # Initialize history with all timesteps as the current obs.\n",
    "    if not 't' in history:\n",
    "        history['t-3'] = model_obs\n",
    "        history['t-2'] = model_obs\n",
    "        history['t-1'] = model_obs\n",
    "        history['t'] = model_obs\n",
    "\n",
    "    # Move history back one timestep and add latest obs.\n",
    "    history['t-3'] = history['t-2']\n",
    "    history['t-2'] = history['t-1']\n",
    "    history['t-1'] = history['t']\n",
    "    history['t'] = model_obs\n",
    "    \n",
    "def prepare_model_observation(env_obs):\n",
    "    state_desc = env_obs\n",
    "    prosthetic = 'pros_foot_r' in state_desc['body_pos']\n",
    "    target_vel_x = 3\n",
    "    target_vel_z = 0\n",
    "\n",
    "    lower_order = []\n",
    "    higher_order = []\n",
    "    \n",
    "    cog = {\n",
    "        'pos': np.array(state_desc['misc']['mass_center_pos']),\n",
    "        'vel': np.array(state_desc['misc']['mass_center_vel']),\n",
    "        'acc': np.array(state_desc['misc']['mass_center_acc']),\n",
    "    }\n",
    "    for body_part in [\"head\",\"torso\",\"pelvis\",\"femur_l\",\"femur_r\",\"tibia_l\",\"tibia_r\",\"pros_tibia_r\",\"talus_l\",\"talus_r\",\"toes_l\",\"toes_r\",\"pros_foot_r\"]:\n",
    "        if self.prosthetic and body_part in [\"toes_r\",\"tibia_r\",\"talus_r\"]:\n",
    "            res += [0] * 9\n",
    "            continue\n",
    "        if not self.prosthetic and body_part in [\"pros_foot_r\",\"pros_tibia_r\"]:\n",
    "            res += [0] * 9\n",
    "            continue\n",
    "        lower_order += list(np.array(state_desc[\"body_pos\"][body_part][0:2]) - cog['pos'])\n",
    "        lower_order += list((np.array(state_desc[\"body_vel\"][body_part][0:2]) - cog['vel']) / cog['vel'])\n",
    "        higher_order += list((np.array(state_desc[\"body_acc\"][body_part][0:2]) - cog['acc']) / cog['acc'])\n",
    "        lower_order += state_desc[\"body_pos_rot\"][body_part][0:2]\n",
    "        lower_order += state_desc[\"body_vel_rot\"][body_part][0:2]\n",
    "        higher_order += state_desc[\"body_acc_rot\"][body_part][0:2]\n",
    "\n",
    "    for joint in [\"ankle_l\",\"ankle_r\",\"back\",\"hip_l\",\"hip_r\",\"knee_l\",\"knee_r\"]:\n",
    "        lower_order += state_desc[\"joint_pos\"][joint]\n",
    "        lower_order += state_desc[\"joint_vel\"][joint]\n",
    "        higher_order += state_desc[\"joint_acc\"][joint]\n",
    "\n",
    "    for muscle in sorted(state_desc[\"muscles\"].keys()):\n",
    "        higher_order += [state_desc[\"muscles\"][muscle][\"activation\"]]\n",
    "        higher_order += [state_desc[\"muscles\"][muscle][\"fiber_length\"]]\n",
    "        higher_order += [state_desc[\"muscles\"][muscle][\"fiber_velocity\"]]\n",
    "\n",
    "    # TODO: difference between COG velocity and target velocity\n",
    "    higher_order += [cog['vel'][0] - target_vel_x, cog['vel'][2] - target_vel_z]\n",
    "    higher_order += cog['acc']\n",
    "        \n",
    "    add_partial_obs_to_history(np.array(higher_order))\n",
    "    return np.concatenate(lower_order, history['t'], history['t-1'], history['t-2'], history['t-3'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action hacking\n",
    "\n",
    "- binary activations?  Or several \"bins\"?\n",
    "- Frameskip?\n",
    "- Muscles must remain \"active\" for at least 10 frames once activated?  Randomized?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_env_action(model_action):\n",
    "    \n",
    "    # TODO \n",
    "    \n",
    "    return model_action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Hacking (Custom \"done\" criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_abort_episode(env):\n",
    "    observation = env.get_state_desc()\n",
    "#     print((np.array(observation['body_pos_rot']['torso'])*180/math.pi > 60).any())\n",
    "#     if observation['body_pos_rot']['torso'][2] < -0.2:\n",
    "#         return True\n",
    "    if observation['body_pos']['head'][0] - observation['body_pos']['pelvis'][0] < -.1:\n",
    "        print('Aborting episode due to head being > .1m behind the pelvis.')\n",
    "        return True\n",
    "    if np.fabs(observation['body_pos']['head'][2]) > 0.5:\n",
    "        print('Aborting episode due to head being > 0.5m away from centerline.')\n",
    "        return True\n",
    "    rewards = compute_rewards(observation)\n",
    "    if sum(rewards.values()) < -10:\n",
    "        print('Aborting episode due to negative score (< -10).')\n",
    "        return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Hacking\n",
    "\n",
    "- Survival reward\n",
    "- Lean forward reward (to avod models which had torso too upright) (may need to be based on speed)\n",
    "- Reward for minimizing sideways velocity?\n",
    "- Reward for minimizing vertical velocity (COG)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rewards(obs):\n",
    "    if type(obs) != dict:\n",
    "        raise ValueError('obs must be a dict (project=False)')\n",
    "\n",
    "    target_vel_x = 3\n",
    "    target_vel_z = 0\n",
    "    eps = 1e-8\n",
    "\n",
    "    target_vel_theta = -np.arctan(target_vel_z/(target_vel_x+eps)) if target_vel_x >= 0 else np.pi - np.arctan(target_vel_z/(target_vel_x+eps))\n",
    "\n",
    "    # Parabolic reward/penalty for tracking a target value within some tolerance. \n",
    "    # Unit reward: 1 at target value, 0 at `tolerance` distance from target value, negative outside of `tolerance`.\n",
    "    # Multiply by desired scale factor / magnitude. Don't multiply by too large of a scale factor — it also amplifies the slope.\n",
    "    # Make the tolerance bigger than you think — it makes the slope more gradual / less severe.\n",
    "    # `tolerance`: reward is positive if within `tolerance` of target value, else negative.\n",
    "    val_diff_reward = lambda val, target, tolerance: (1 - ((val - target)/tolerance)**2)\n",
    "    def radians_diff_wrapped(a1, a2):\n",
    "        # Make both angles within (-2pi, 2pi)\n",
    "        a1, a2 = a1 % (2*np.pi), a2 % (2*np.pi)\n",
    "        # Make both angles positive -- within [0, 2pi)\n",
    "        a1 = a1 + 2*np.pi if a1 < 0 else a1\n",
    "        a2 = a2 + 2*np.pi if a2 < 0 else a2\n",
    "        # Make a1 the smaller angle\n",
    "        a1, a2 = min(a1, a2), max(a1, a2)\n",
    "        # Make sure a1 is within pi of a2 — two angles can't be greater than pi apart in relative (wrapped) sense.\n",
    "        a1 = a1 + 2*np.pi if a2 - a1 > np.pi else a1\n",
    "        return np.fabs(a2 - a1)\n",
    "    \n",
    "    penalty += (state_desc[\"body_vel\"][\"pelvis\"][0] - state_desc[\"target_vel\"][0])**2\n",
    "    return {\n",
    "        'survival': 2,\n",
    "        'target_velocity_x': 3 * val_diff_reward(obs['misc']['mass_center_vel'][0], target_vel_x, 5), # 3 at target velocity, 0 at 3m/s off-target, then negative\n",
    "        'target_velocity_z': 3 * val_diff_reward(obs['misc']['mass_center_vel'][2], target_vel_z, 5), # 3 at target velocity, 0 at 3m/s off-target, then negative\n",
    "        'head_velocity_x': 2 * val_diff_reward(obs['body_vel']['head'][0], target_vel_x, 5), # 2 at target velocity, 0 at 3m/s off-target, then negative\n",
    "        'head_velocity_z': 2 * val_diff_reward(obs['body_vel']['head'][2], target_vel_z, 5), # 2 at target velocity, 0 at 3m/s off-target, then negative\n",
    "        'lean_forward_x': 5 * val_diff_reward(obs['body_pos']['head'][0] - obs['body_pos']['pelvis'][0], .1 * target_vel_x, .4), # head in front of pelvis from perspective of velocity vector\n",
    "        'lean_forward_z': 5 * val_diff_reward(obs['body_pos']['head'][2] - obs['body_pos']['pelvis'][2], .1 * target_vel_z, .4), # head in front of pelvis from perspective of velocity vector\n",
    "        'hips_squared': 5 * val_diff_reward(radians_diff_wrapped(target_vel_theta, obs['body_pos_rot']['pelvis'][1]), 0, np.pi),\n",
    "        'knee_bent_l': 5 * val_diff_reward(obs['joint_pos']['knee_l'][0], 60*np.pi/180, np.pi), # goal range of roughly [0,120] degrees\n",
    "        'knee_bent_r': 5 * val_diff_reward(obs['joint_pos']['knee_r'][0], 60*np.pi/180, np.pi), # goal range of roughly [0,120] degrees\n",
    "        'low_y_vel_knee_l': 5 * val_diff_reward(obs['body_vel']['knee_l'][1], 0, 1),\n",
    "        'low_y_vel_knee_r': 5 * val_diff_reward(obs['body_vel']['knee_r'][1], 0, 1),\n",
    "        'low_y_vel_toes_l': 5 * val_diff_reward(obs['body_vel']['toes_l'][1], 0, 1),\n",
    "        'low_y_vel_pros_foot_r': 5 * val_diff_reward(obs['body_vel']['pros_foot_r'][1], 0, 1),\n",
    "        'low_y_vel_pelvis': 5 * val_diff_reward(obs['body_vel']['pelvis'][1], 0, 1),\n",
    "        'low_y_vel_head': 5 * val_diff_reward(obs['body_vel']['head'][1], 0, 1),\n",
    "        'knees_opposite_joint_vel': 10 * val_diff_reward(obs['joint_vel']['knee_l'][0], -obs['joint_vel']['knee_r'][0], np.pi), # The left knee should be opening when the right knee is closing, and vice versa\n",
    "        'feet_behind_mass_x': 5 * val_diff_reward(obs['misc']['mass_center_pos'][0] - (obs['body_pos']['toes_l'][0] + obs['body_pos']['pros_foot_r'][0])/2, .1 * target_vel_x, .4),\n",
    "        'feet_behind_mass_z': 5 * val_diff_reward(obs['misc']['mass_center_pos'][2] - (obs['body_pos']['toes_l'][2] + obs['body_pos']['pros_foot_r'][2])/2, .1 * target_vel_z, .4),\n",
    "        'one_foot_off_ground': 0,\n",
    "        'femurs_parallel': 0,\n",
    "        'absolute_foot_velocity': 0, # should be 0 at ground, 2x body velocity above ground\n",
    "        'forefoot_strike': 0,\n",
    "    }    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns average reward\n",
    "def run_episode(policy, custom_done_fn=None):\n",
    "    obs = env.reset(**env_step_kwargs)\n",
    "    obs = env.reset(**env_step_kwargs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = policy.select_action(prepare_model_observation(obs))\n",
    "        action = prepare_env_action(action)\n",
    "        obs, reward, done, _ = env.step(action, **env_step_kwargs)\n",
    "        \n",
    "        # We don't use the custom rewards here because we want to evaluate our progress against the environment's reward.\n",
    "        # obs_dict = env.get_state_desc()\n",
    "        # custom_rewards = compute_rewards(obs_dict)\n",
    "        # total_rewared += reward + sum(custom_rewards.values())\n",
    "\n",
    "        total_reward += reward\n",
    "        if not done and callable(custom_done_fn):\n",
    "            done = custom_done_fn(env)\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10, custom_done_fn=None):\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        avg_reward += run_episode(policy, custom_done_fn=custom_done_fn)\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Evaluation over %d episodes: %f\" % (eval_episodes, avg_reward))\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = ProstheticsEnv(visualize=False, integrator_accuracy=CONFIG['env']['integrator_accuracy'])\n",
    "env_step_kwargs = {'project': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = int(env.action_space.high[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-60b259a28645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-n1 -r1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'evaluations = [evaluate_policy(policy, custom_done_fn=should_abort_episode)]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2165\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2167\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-929c34d92a4f>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(policy, eval_episodes, custom_done_fn)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mavg_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_done_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_done_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mavg_reward\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0meval_episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-929c34d92a4f>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(policy, custom_done_fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0menv_step_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5660030d6b9a>\u001b[0m in \u001b[0;36mprepare_observation\u001b[0;34m(state_desc)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbody_part\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"pelvis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"head\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"torso\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"toes_l\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"toes_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"talus_l\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"talus_r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprosthetic\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbody_part\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"toes_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"talus_r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "evaluations = [evaluate_policy(policy, custom_done_fn=should_abort_episode)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 95 Episode Num: 1 Episode T: 95 Reward: -543.9680248189658\n",
      "Total T: 192 Episode Num: 2 Episode T: 97 Reward: -559.8757033004392\n",
      "Total T: 288 Episode Num: 3 Episode T: 96 Reward: -540.4079581627547\n",
      "Total T: 386 Episode Num: 4 Episode T: 98 Reward: -563.3346533803907\n",
      "Total T: 482 Episode Num: 5 Episode T: 96 Reward: -555.292968682621\n",
      "Total T: 578 Episode Num: 6 Episode T: 96 Reward: -550.2183485309434\n",
      "Total T: 674 Episode Num: 7 Episode T: 96 Reward: -541.1373096478648\n",
      "Total T: 770 Episode Num: 8 Episode T: 96 Reward: -563.6875861703782\n",
      "Total T: 867 Episode Num: 9 Episode T: 97 Reward: -568.2517473832543\n",
      "Total T: 964 Episode Num: 10 Episode T: 97 Reward: -565.1752790861794\n",
      "Total T: 1060 Episode Num: 11 Episode T: 96 Reward: -559.6357161388767\n",
      "Total T: 1158 Episode Num: 12 Episode T: 98 Reward: -565.3874770336167\n",
      "Total T: 1253 Episode Num: 13 Episode T: 95 Reward: -550.3142296671739\n",
      "Total T: 1350 Episode Num: 14 Episode T: 97 Reward: -567.3470015031003\n",
      "Total T: 1446 Episode Num: 15 Episode T: 96 Reward: -558.9980211715862\n",
      "Total T: 1544 Episode Num: 16 Episode T: 98 Reward: -561.2427793484804\n",
      "Total T: 1641 Episode Num: 17 Episode T: 97 Reward: -567.264610316939\n",
      "Total T: 1738 Episode Num: 18 Episode T: 97 Reward: -556.4026031305953\n",
      "Total T: 1835 Episode Num: 19 Episode T: 97 Reward: -571.2502758776874\n",
      "Total T: 1932 Episode Num: 20 Episode T: 97 Reward: -557.3423163558119\n",
      "Total T: 2027 Episode Num: 21 Episode T: 95 Reward: -540.6127857904354\n",
      "Total T: 2124 Episode Num: 22 Episode T: 97 Reward: -554.3972977289874\n",
      "Total T: 2221 Episode Num: 23 Episode T: 97 Reward: -550.5748739673128\n",
      "Total T: 2317 Episode Num: 24 Episode T: 96 Reward: -558.0022698190755\n",
      "Total T: 2414 Episode Num: 25 Episode T: 97 Reward: -564.0344517951559\n",
      "Total T: 2512 Episode Num: 26 Episode T: 98 Reward: -557.2581094590789\n",
      "Total T: 2609 Episode Num: 27 Episode T: 97 Reward: -546.9053603815606\n",
      "Total T: 2704 Episode Num: 28 Episode T: 95 Reward: -525.4227271700519\n",
      "Total T: 2801 Episode Num: 29 Episode T: 97 Reward: -541.7917336865661\n",
      "Total T: 2897 Episode Num: 30 Episode T: 96 Reward: -543.1363233875209\n",
      "Total T: 2993 Episode Num: 31 Episode T: 96 Reward: -548.6983148532346\n",
      "Total T: 3089 Episode Num: 32 Episode T: 96 Reward: -563.5576043014393\n",
      "Total T: 3185 Episode Num: 33 Episode T: 96 Reward: -533.9130775698143\n",
      "Total T: 3281 Episode Num: 34 Episode T: 96 Reward: -561.9090823242426\n",
      "Total T: 3377 Episode Num: 35 Episode T: 96 Reward: -568.3176687640915\n",
      "Total T: 3473 Episode Num: 36 Episode T: 96 Reward: -533.8416748659179\n",
      "Total T: 3571 Episode Num: 37 Episode T: 98 Reward: -547.1665193311045\n",
      "Total T: 3667 Episode Num: 38 Episode T: 96 Reward: -553.4659954903091\n",
      "Total T: 3763 Episode Num: 39 Episode T: 96 Reward: -560.5585748038682\n",
      "Total T: 3860 Episode Num: 40 Episode T: 97 Reward: -534.3575368879249\n",
      "Total T: 3956 Episode Num: 41 Episode T: 96 Reward: -564.5976741295143\n",
      "Total T: 4055 Episode Num: 42 Episode T: 99 Reward: -545.3122711407883\n",
      "Total T: 4154 Episode Num: 43 Episode T: 99 Reward: -565.1691980483185\n",
      "Total T: 4252 Episode Num: 44 Episode T: 98 Reward: -563.4805621063803\n",
      "Total T: 4348 Episode Num: 45 Episode T: 96 Reward: -548.3472324629257\n",
      "Total T: 4445 Episode Num: 46 Episode T: 97 Reward: -548.0537349555115\n",
      "Total T: 4541 Episode Num: 47 Episode T: 96 Reward: -559.68220206132\n",
      "Total T: 4637 Episode Num: 48 Episode T: 96 Reward: -558.9869129059764\n",
      "Total T: 4733 Episode Num: 49 Episode T: 96 Reward: -556.1451694252494\n",
      "Total T: 4829 Episode Num: 50 Episode T: 96 Reward: -569.7492483419212\n",
      "Total T: 4928 Episode Num: 51 Episode T: 99 Reward: -555.8403097595716\n",
      "Total T: 5024 Episode Num: 52 Episode T: 96 Reward: -549.6248684561863\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "std::exception in 'SimTK::State const & OpenSim::Manager::integrate(double)': SimTK Exception thrown at AbstractIntegratorRep.cpp:428:\n  Integrator step failed at time 0.35336897974972997 apparently because:\nSimTK Exception thrown at AbstractIntegratorRep.cpp:547:\n  Error detected by Simbody method AbstractIntegrator::takeOneStep(): Unable to advance time past 0.353369.\n  (Required condition 't1 > t0' was not met.)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-58444a32e037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimesteps_since_eval\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtimesteps_since_eval\u001b[0m \u001b[0;34m%=\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mevaluations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_done_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshould_abort_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHECKPOINTS_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8c3e9b34b480>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(policy, eval_episodes, custom_done_fn)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mavg_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_done_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_done_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mavg_reward\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0meval_episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8c3e9b34b480>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(policy, custom_done_fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_done_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/osim/env/osim.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, project)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_state_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mosim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactuate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mosim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/osim/env/osim.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# Integrate till the new endtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mistep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nips-2018-ai-for-prosthetics-fCqIkKV7/lib/python3.6/site-packages/opensim/simulation.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, finalTime)\u001b[0m\n\u001b[1;32m  38432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  38433\u001b[0m         \"\"\"\n\u001b[0;32m> 38434\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_simulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mManager_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  38435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  38436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: std::exception in 'SimTK::State const & OpenSim::Manager::integrate(double)': SimTK Exception thrown at AbstractIntegratorRep.cpp:428:\n  Integrator step failed at time 0.35336897974972997 apparently because:\nSimTK Exception thrown at AbstractIntegratorRep.cpp:547:\n  Error detected by Simbody method AbstractIntegrator::takeOneStep(): Unable to advance time past 0.353369.\n  (Required condition 't1 > t0' was not met.)\n"
     ]
    }
   ],
   "source": [
    "while total_timesteps < CONFIG['training']['max_timesteps']:\n",
    "    if done: \n",
    "        if total_timesteps != 0: \n",
    "            print(f\"Total T: {total_timesteps} Episode Num: {episode_num} Episode T: {episode_timesteps} Reward: {episode_reward}\")\n",
    "            if CONFIG['model']['architecture'] == \"TD3\":\n",
    "                policy.train(\n",
    "                    replay_buffer, \n",
    "                    episode_timesteps, \n",
    "                    CONFIG['training']['batch_size'], \n",
    "                    CONFIG['training']['discount'], \n",
    "                    CONFIG['training']['tau'], \n",
    "                    CONFIG['training']['policy_noise'], \n",
    "                    CONFIG['training']['noise_clip'], \n",
    "                    CONFIG['training']['policy_freq']\n",
    "                )\n",
    "            else: \n",
    "                policy.train(replay_buffer, episode_timesteps, CONFIG['training']['batch_size'], CONFIG['training']['discount'], CONFIG['training']['tau'])\n",
    "        \n",
    "        # Evaluate episode\n",
    "        if timesteps_since_eval >= CONFIG['training']['eval_freq']:\n",
    "            timesteps_since_eval %= CONFIG['training']['eval_freq']\n",
    "            evaluations.append(evaluate_policy(policy, custom_done_fn=should_abort_episode))\n",
    "            torch.save(policy.state_dict(), CHECKPOINTS_DIR/file_name)\n",
    "        \n",
    "        # Reset environment\n",
    "        obs = env.reset(**env_step_kwargs)\n",
    "        obs_dict = env.get_state_desc()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "    \n",
    "    # Select action randomly or according to policy\n",
    "    if total_timesteps < CONFIG['training']['start_timesteps']:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = policy.select_action(prepare_model_observation(obs))\n",
    "        if CONFIG['training']['expl_noise'] != 0: \n",
    "            action = (action + np.random.normal(0, CONFIG['training']['expl_noise'], size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "    # Perform action\n",
    "    action = prepare_env_action(action)\n",
    "    new_obs, reward, done, _ = env.step(action, **env_step_kwargs)\n",
    "    new_obs_dict = env.get_state_desc()\n",
    "\n",
    "    if not done:\n",
    "        done = should_abort_episode(env)\n",
    "    done_bool = 0 if episode_timesteps + 1 == CONFIG['training']['max_episode_steps'] else float(done)\n",
    "\n",
    "    custom_rewards = compute_rewards(obs_dict)\n",
    "    episode_reward += reward + sum(custom_rewards.values())\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add((obs_dict, new_obs_dict, action, reward, done_bool, episode_num))\n",
    "\n",
    "    obs = new_obs\n",
    "    obs_dict = new_obs_dict\n",
    "\n",
    "    episode_timesteps += 1\n",
    "    total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutdown scheduled for Wed 2018-08-29 06:23:47 UTC, use 'shutdown -c' to cancel.\r\n"
     ]
    }
   ],
   "source": [
    "!sudo shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosthetics",
   "language": "python",
   "name": "prosthetics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
